---
title: "Efficient R"
author: "Annika Kufrovich"
date: "January 8, 2019"
output: html_document
---

## Notes

* Benchmarking 
    + Construct a function to be benchmarked 
    + Time the function under different scenarios 
    + Using system.time on a function gives the user, system, and elapsed time 
    + User time is the CPU time charged for the execution of user instructions 
    + System time is the CPU time charged for execution by the system on behalf of the calling process
    + Elapsed time is the sum of the user and system time 
    + `microbenchmark()` function from the microbenchmark package can run multiple functions multiple times to see how long they take and compare them to eachother 
    + The benchmarkme package allows you to run standard R code and see how yours stacks up against other R users through the `benchmark_sd()` function and then plotting the result. You can upload your result after 
* Memory Allocation, don't ever grow a vector
* The importance of vectorizing code 
    + A general rule is to access optimized C or FORTRAN code as quickly as possible, the fewer functions called, the better 
    + Seems to me like you should generally write as few lines of code as possible. A clunky function is not always the best way 
* Data frames and Matrices, pretty much just use a matrix when you can 
* Code Profiling 
    + General Idea is to run code and, every few milliseconds, record what is being executed 
    + `Rprof` works for this but is tricky to use 
    + The profvis package is better 
    + profiling ggplot is difficult, easier to profile base R 

###Day 2 Notes 
* CPU's now have multiple cores 
* By default, R only uses 1 core 
    + You can use the parallel package to use multiple cores for sections of code as well as detect how many cores your computer has 
    + However, not all code can be run parallel by design, good rule of thumb is to figure out if you can run the code/loop forwards and backwards and still get the same result
    + This can also be read whether each iteration is independent of the last in your code
    + If you want to ue your computer for other things while running coe, like sending emails, don't use all your cores, use total number of cores - 1 
    + Once you asign the number of cores to be used to a variable, make a cluster by putting you cores variable into a cluster with the `makeCluster()` function and assign that to a new variable
    + Then, if possible switch the function you're using to its parallel alternative, this will take in your cluter variable
    + once done, stop the cluster with the `stopCluster` function
    + if something is already fast multiple cores coul slow the function down

## Practice/Application

```{r, include=FALSE}
library(microbenchmark)
library(benchmarkme)
library(profvis)
library(dplyr)
library(parallel)
```

```{r}
##res <- benchmark_std(runs = 3)
##plot(res)
```

takes a bit to run this code, Apparently my laptop is slow :(


oh well, lets try my function from yesterday on the smallest county in Florida, Liberty County
```{r, message=FALSE, warning=FALSE}
matching_dfs_addcol <- function(newdf, olddf) {
  matching <- semi_join(olddf, newdf)
  matching_col <- matching %>%
    mutate(matched = TRUE)
  not_matching <- anti_join(newdf, olddf)
  not_matching_col <- not_matching %>%
    mutate(matched = FALSE)
  combined_df <- full_join(matching_col, not_matching_col)
  combined_df
}

old_lib <- read.delim("C:/Users/TeenieTiny/Desktop/2nd Year/Data-Old/Voter_Registration_20180410/20180410_VoterDetail/LIB_20180410.txt", header=FALSE)


new_lib <- read.delim("C:/Users/TeenieTiny/Desktop/2nd Year/Data/Voter_Registration_20181113/20181113_VoterDetail/LIB_20181113.txt", header=FALSE)


#old one is not currently geo-coded, adding fake lon, lat columns
old_lib_lonlat <- old_lib %>% 
  mutate(lon = 80) %>% 
  mutate(lat = 90)

new_lib$V25 <- as.integer(new_lib$V25)
new_lib$V26 <- as.integer(new_lib$V26)
new_lib$V30 <- as.integer(new_lib$V30)
new_lib$V31 <- as.integer(new_lib$V31)
new_lib$V32 <- as.integer(new_lib$V32)
new_lib$V33 <- as.integer(new_lib$V33)
new_lib$V34 <- as.integer(new_lib$V34)
new_lib$V28 <- NULL
old_lib_lonlat$V28 <- NULL




new_libvoters <- matching_dfs_addcol(new_lib, old_lib_lonlat)
summary(new_libvoters$matched)
```


###Day 2 Practice/Application

maybe I should adjust this to only look at certain columns since no one came up as "matched", columns kept include columns related to voterID, name, and address

```{r, message=FALSE, warning=FALSE}
nl_small <- new_lib %>%
  select(V2:V12)
ol_small <- old_lib_lonlat %>%
  select(V2:V12, lon, lat)

nl_voters_retry <- matching_dfs_addcol(nl_small, ol_small)
summary(nl_voters_retry$matched)

```

It works! there was a lot of warning about coercing the "Factor" columns into character columns so I'll remove the name columns and set the address colums to character columns.

```{r, message=FALSE, warning=FALSE}
nl_small2 <- nl_small %>%
  select(V2, V7:V12)
nl_small2$V7 <- as.character(nl_small2$V7)
nl_small2$V8 <- as.character(nl_small2$V8)
nl_small2$V9 <- as.character(nl_small2$V9)
nl_small2$V10 <- as.character(nl_small2$V10)
nl_small2$V11 <- as.character(nl_small2$V11)
nl_small2$V12 <- as.character(nl_small2$V12)

ol_small2 <- ol_small %>%
  select(V2, V7:V12)
ol_small2$V7 <- as.character(ol_small2$V7)
ol_small2$V8 <- as.character(ol_small2$V8)
ol_small2$V9 <- as.character(ol_small2$V9)
ol_small2$V10 <- as.character(ol_small2$V10)
ol_small2$V11 <- as.character(ol_small2$V11)
ol_small2$V12 <- as.character(ol_small2$V12)


nl_voters_retry2 <- matching_dfs_addcol(nl_small2, ol_small2)
summary(nl_voters_retry2$matched)
```

now to profile my function

```{r, message=FALSE, warning=FALSE}
profvis(nl_voters_retry2 <- matching_dfs_addcol(nl_small2, ol_small2))
```

seems like I don't have any bottlenecks yet, function is doing well




## Goals for Tomorrow
* Play around more with voter data and my function until I get a reasonable amount of matches, then benchmark my code
* finish Efficient R code course and maybe start parallel programming course?