---
title: "Efficient R"
author: "Annika Kufrovich"
date: "January 8, 2019"
output: html_document
---

## Notes

* Benchmarking 
    + Construct a function to be benchmarked 
    + Time the function under different scenarios 
    + Using system.time on a function gives the user, system, and elapsed time 
    + User time is the CPU time charged for the execution of user instructions 
    + System time is the CPU time charged for execution by the system on behalf of the calling process
    + Elapsed time is the sum of the user and system time 
    + `microbenchmark()` function from the microbenchmark packae can run multiple functions multiple times to see how long they take and compare them to eachother 
    + The benchmarkme package allows you to run standard R code and see how yours stacks up against other R users through the `benchmark_sd()` function and then plotting the result. You can upload your result after 
* Memory Allocation, don't ever grow a vector
* The importance of vectorizing code 
    + A general rule is to access optimized C or FORTRAN code as quickly as possible, the fewer functions called, the better 
    + Seems to me like you should generally write as few lines of code as possible. A clunky function is not always the best way 
* Data frames and Matrices, pretty much just use a matrix when you can 
* Code Profiling 
    + General Idea is to run code and, every few milliseconds, record what is being executed 
    + `Rprof` works for this but is tricky to use
    + The profvis package is better
    + profiling ggplot is difficult, easier to profile base R
* 

## Practice/Application

```{r, include=FALSE}
library(microbenchmark)
library(benchmarkme)
library(profvis)
library(dplyr)
```

```{r}
##res <- benchmark_std(runs = 3)
##plot(res)
```

takes a bit to run this code, Apparently my laptop is slow :(


oh well, lets try my function from yesterday on the smallest county in Florida, Liberty County
```{r, message=FALSE, warning=FALSE}
matching_dfs_addcol <- function(newdf, olddf) {
  matching <- semi_join(olddf, newdf)
  matching_col <- matching %>%
    mutate(matched = TRUE)
  not_matching <- anti_join(newdf, olddf)
  not_matching_col <- not_matching %>%
    mutate(matched = FALSE)
  combined_df <- full_join(matching_col, not_matching_col)
  combined_df
}

old_lib <- read.delim("C:/Users/TeenieTiny/Desktop/2nd Year/Data-Old/Voter_Registration_20180410/20180410_VoterDetail/LIB_20180410.txt", header=FALSE)


new_lib <- read.delim("C:/Users/TeenieTiny/Desktop/2nd Year/Data/Voter_Registration_20181113/20181113_VoterDetail/LIB_20181113.txt", header=FALSE)


#old one is not currently geo-coded, adding fake lon, lat columns
old_lib_lonlat <- old_lib %>% 
  mutate(lon = 80) %>% 
  mutate(lat = 90)

new_lib$V25 <- as.integer(new_lib$V25)
new_lib$V26 <- as.integer(new_lib$V26)
new_lib$V30 <- as.integer(new_lib$V30)
new_lib$V31 <- as.integer(new_lib$V31)
new_lib$V32 <- as.integer(new_lib$V32)
new_lib$V33 <- as.integer(new_lib$V33)
new_lib$V34 <- as.integer(new_lib$V34)
new_lib$V28 <- NULL
old_lib_lonlat$V28 <- NULL


new_libvoters <- matching_dfs_addcol(new_lib, old_lib_lonlat)
summary(new_libvoters$matched)
```


maybe I should adjust this to only look at certain columns sice no one came up as "matched"


## Goals for Tomorrow
* Play around more with voter data and my function until I get a reasonable amount of matches, then benchmark my code
* finish Efficient R code course and maybe start parallel programming course?